{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to convert features to binomial\n",
    "# data is the dataset to be used for the algorithm\n",
    "# features is a list of the indicies for the features\n",
    "# return data_binomial, which is the dataset with the features converted to binomial\n",
    "def binomial_features(data, features):\n",
    "    data_binomial = copy.deepcopy(data)\n",
    "    for index in features:\n",
    "        column_name = data_binomial.columns[index]\n",
    "        mean = data_binomial[column_name].mean()\n",
    "        data_ltm = data_binomial[column_name] < mean\n",
    "        data_binomial.loc[data_ltm, column_name] = 0\n",
    "        data_gtm = data_binomial[column_name] >= mean\n",
    "        data_binomial.loc[data_gtm, column_name] = 1\n",
    "    return data_binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Datasets](#Datasets):\n",
    "\n",
    "[Iris](#Iris)- 3 classes    \n",
    "[Glass](#Glass)- 7 classes  \n",
    "[Breast Cancer](#Breast_Cancer)- 2 classes  \n",
    "[Votes](#Votes)- 2 classes  \n",
    "[Soybean](#Soybean)- 4 classes  \n",
    "\n",
    "[Cross Validation](#Cross_Val)  \n",
    "[Logistic Regression](#Log_Regress)  \n",
    "[Naive Bayes](#Naive_Bayes)  \n",
    "[Models](#Models)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Datasets\"></a>\n",
    "# Datasets\n",
    "\n",
    "<a id=\"Iris\"></a>\n",
    "## Iris\n",
    "\n",
    "Dummies for different Iris Classes:  \n",
    "iris_class_Iris-setosa       \n",
    "iris_class_Iris-versicolor    \n",
    "iris_class_Iris-virginica  \n",
    "Target Class Indices: [5,6,7]  \n",
    "Feature Indices: [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv( \"iris.data\", header=None, names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width','iris_class'])\n",
    "iris_log = pd.read_csv( \"iris.data\", header=None, names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width','iris_class'])\n",
    "iris = pd.concat([iris, pd.get_dummies(iris[\"iris_class\"],prefix = 'iris_class')], axis=1)\n",
    "iris_log = pd.concat([iris_log, pd.get_dummies(iris[\"iris_class\"],prefix = 'iris_class')], axis=1)\n",
    "iris_features = [0,1,2,3]\n",
    "iris_binomial = binomial_features(iris, iris_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Glass\"></a>\n",
    "## Glass\n",
    "\n",
    "Class Indices: [11,12,13,14,15,16]  \n",
    "Feature Indices: [1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = pd.read_csv( \"glass.data\", header=None, names = ['ID','RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','glass_class'])\n",
    "glass_log = pd.read_csv( \"glass.data\", header=None, names = ['ID','RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','glass_class'])\n",
    "glass = pd.concat([glass, pd.get_dummies(glass[\"glass_class\"],prefix = 'glass_class')], axis=1)\n",
    "glass_log = pd.concat([glass_log, pd.get_dummies(glass[\"glass_class\"],prefix = 'glass_class')], axis=1)\n",
    "glass_features = [1,2,3,4,5,6,7,8,9]\n",
    "glass_binomial = binomial_features(glass, glass_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Breast_Cancer\"></a>\n",
    "## Breast Cancer\n",
    "\n",
    "Remove 16 records with missing data.  \n",
    "Class 2 changed to 1  \n",
    "Class 4 changed to 0  \n",
    "Class Index: 10  \n",
    "Features: [1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = pd.read_csv( \"breast-cancer-wisconsin.data\", header=None, names = ['ID','clump_thick','unif_cell_size','unif_cell_shape','marg_adh','single_epit_cell_size','bare_nuclei','bland_chromatin','normal_nucleoli','mitosis','cancer_class'])\n",
    "breast_cancer_log = pd.read_csv( \"breast-cancer-wisconsin.data\", header=None, names = ['ID','clump_thick','unif_cell_size','unif_cell_shape','marg_adh','single_epit_cell_size','bare_nuclei','bland_chromatin','normal_nucleoli','mitosis','cancer_class'])\n",
    "breast_cancer['cancer_class'] = breast_cancer['cancer_class'].map({2: 1, 4: 0})\n",
    "breast_cancer_log['cancer_class'] = breast_cancer_log['cancer_class'].map({2: 1, 4: 0})\n",
    "breast_cancer=breast_cancer[breast_cancer.bare_nuclei != '?']\n",
    "breast_cancer.bare_nuclei = breast_cancer.bare_nuclei.astype(int)\n",
    "breast_cancer_log=breast_cancer_log[breast_cancer_log.bare_nuclei != '?']\n",
    "breast_cancer_log.bare_nuclei = breast_cancer_log.bare_nuclei.astype(int)\n",
    "breast_cancer_features = [1,2,3,4,5,6,7,8,9]\n",
    "breast_cancer_target_class = 10\n",
    "breast_cancer_binomial = binomial_features(breast_cancer, breast_cancer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Votes\"></a>\n",
    "## Votes\n",
    "\n",
    "Changed all ? to 'n', 'n' to 0, 'y' to 1.  \n",
    "Democrats = 1  \n",
    "Republicans = 0  \n",
    "Class Index: 0  \n",
    "Feature Indices: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = pd.read_csv( \"house-votes-84.data\", header=None, names = ['votes_class','handicapped_infants','water_project','adoption_budget_resol','physician_fee_freeze','el_salv_aid','rel_groups_in_schools','anti_satellite_test_ban','aid_to_nica_contras','mx_missile','immigration','synfuels','education_spending','superfund_right_to_sue','crime','duty_free_exports','export_admin_act_south_africa'])\n",
    "votes = votes.replace('y', 1)\n",
    "votes = votes.replace('n', 0)\n",
    "votes = votes.replace('?', 0)\n",
    "votes = votes.replace('democrat', 1)\n",
    "votes = votes.replace('republican', 0)\n",
    "votes_features = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Soybean\"></a>\n",
    "## Soybean\n",
    "\n",
    "Class Indices: [35,36,37,38]  \n",
    "Feature Indices: [0,1,2,3,4,5,6,7,8,10,18,19,20,21,22,23,24,25,26,33]  \n",
    "\n",
    "Not using features due to no info:  \n",
    "seed  \n",
    "mold_growth  \n",
    "seed_discolor  \n",
    "seed_size  \n",
    "shriveling  \n",
    "leaves  \n",
    "leaf_shread  \n",
    "leaf_malf  \n",
    "leaf_mild  \n",
    "germination  \n",
    "leafspots_marg  \n",
    "leafspot_size  \n",
    "stem  \n",
    "fruit_spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean = pd.read_csv( \"soybean-small.data\", header=None, names = ['date','plant_stand','precip','temp','hail','crop_hist','area_damaged','severity','seed_tmt','germination','plant_growth','leaves','leafspots_marg','leafspot_size','leaf_shread','leaf_malf','leaf_mild','stem','lodging','stem_cankers','canker_lesion','fruiting_bodies','external_decay','mycelium','int_discolor','sclerotia','fruit_pods','fruit_spots','seed','mold_growth','seed_discolor','seed_size','shriveling','roots','soybean_class'])\n",
    "soybean = pd.concat([soybean, pd.get_dummies(soybean[\"soybean_class\"],prefix = 'soybean_class')], axis=1)\n",
    "soybean_features = [1,2,3,4,5,6,7,8,10,18,19,20,21,22,23,24,25,26,33] \n",
    "soybean_binomial = binomial_features(soybean, soybean_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean_log = pd.read_csv( \"soybean-small.data\", header=None, names = ['date','plant_stand','precip','temp','hail','crop_hist','area_damaged','severity','seed_tmt','germination','plant_growth','leaves','leafspots_marg','leafspot_size','leaf_shread','leaf_malf','leaf_mild','stem','lodging','stem_cankers','canker_lesion','fruiting_bodies','external_decay','mycelium','int_discolor','sclerotia','fruit_pods','fruit_spots','seed','mold_growth','seed_discolor','seed_size','shriveling','roots','soybean_class'])\n",
    "soybean_log = pd.concat([soybean_log, pd.get_dummies(soybean_log[\"soybean_class\"],prefix = 'soybean_class')], axis=1)\n",
    "soybean_log = soybean_log.append(soybean_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Cross_Val\"></a>\n",
    "## 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross Validation\n",
    "# data is the dataset to be used for the algorithm\n",
    "# target_class is the index of the target class feature\n",
    "# features is a list of the indicies for the features to be used to measure distance\n",
    "# Algorithm is which algorithm is chosen to be used\n",
    "def five_fold_cross_val(data, target_class, features, Algorithm):\n",
    "    metrics= []\n",
    "    list_of_indices = list(range(len(data)))\n",
    "    n_data = len(data) #this is the number of records in the data\n",
    "    test_size = int(n_data/5) #this is the number of records in each test set\n",
    "    df = copy.deepcopy(data)\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # a dataframe with shuffled records\n",
    "    df.sort_values(by= df.columns[target_class]) # sorts the data by the target class\n",
    "    \n",
    "    split1 = []\n",
    "    split2 = []\n",
    "    split3 = []\n",
    "    split4 = []\n",
    "    split5 = []\n",
    "    for count in range(test_size):\n",
    "        split1.append(5*count)\n",
    "        split2.append(5*count +1)\n",
    "        split3.append(5*count +2)\n",
    "        split4.append(5*count +3)\n",
    "        split5.append(5*count +4)\n",
    "    \n",
    "    # test split1\n",
    "    split1_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split1):\n",
    "            split1_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes1, model1 = Algorithm(df, split1, split1_train, target_class, features)\n",
    "    output1 = performance(df, split1, test_classes1, target_class) \n",
    "    metrics.append(output1)\n",
    "    \n",
    "    # test split2\n",
    "    split2_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split2):\n",
    "            split2_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes2, model2 = Algorithm(df, split2, split2_train, target_class, features)\n",
    "    output2 = performance(df, split2, test_classes2, target_class)     \n",
    "    metrics.append(output2)\n",
    "    \n",
    "    # test split3\n",
    "    split3_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split3):\n",
    "            split3_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes3, model3 = Algorithm(df, split3, split3_train, target_class, features)\n",
    "    output3 = performance(df, split3, test_classes3, target_class)    \n",
    "    metrics.append(output3)\n",
    "    \n",
    "    # test split4\n",
    "    split4_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split4):\n",
    "            split4_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes4, model4 = Algorithm(df, split4, split4_train, target_class, features)\n",
    "    output4 = performance(df, split4, test_classes4, target_class)    \n",
    "    metrics.append(output4)\n",
    "    \n",
    "    # test split5\n",
    "    split5_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split5):\n",
    "            split5_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes5, model5 = Algorithm(df, split5, split5_train, target_class, features)\n",
    "    output5 = performance(df, split5, test_classes5, target_class)  \n",
    "    metrics.append(output5)  \n",
    "        \n",
    "    return metrics, model1, test_classes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "# data is the dataset to be used for the algorithm\n",
    "# test is a list of indices from data to be used for testing\n",
    "# test_classes is a list with the class for the test dataset that includes the estimate for the target_class\n",
    "# features is a list of the indicies for the features to be used to measure distance\n",
    "# target_class is the index of the target class feature\n",
    "def performance(data, test, test_classes, target_class):\n",
    "    n_data = len(test) # this is the number of records to be tested\n",
    "    curr_perf = 0\n",
    "\n",
    "    # classification error\n",
    "    for record in range(n_data): # record is index in test \n",
    "        test_record = data.iloc[[test[record]]] # pulls record to test\n",
    "        if(test_record.iloc[0,target_class] != test_classes[record]):\n",
    "            curr_perf = curr_perf + 1\n",
    "        # end if\n",
    "    # end for\n",
    "    curr_perf = curr_perf / n_data\n",
    "\n",
    "    return curr_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Log_Regress\"></a>\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# data is the dataset to be used for the algorithm\n",
    "# test is a list of indices from data to be used for testing\n",
    "# train is a list of indices from data to be used for training\n",
    "# target_class is the index of the target class feature\n",
    "# features is a list of the indicies for the features to be used to measure distance\n",
    "# returns target_classes, which is a list that contains the target class calculated for each test index\n",
    "# returns betas, which is a list of the betas trained for the logistic regression model\n",
    "def logistic_regression_train(data, test, train, target_class, features):\n",
    "    # get the normalized featureset for the train data\n",
    "    train_data = copy.deepcopy(data)\n",
    "    train_data = train_data.iloc[train]\n",
    "    train_data = train_data.iloc[:,features]\n",
    "    train_data = train_data.values\n",
    "    feature_data = normalize(train_data)\n",
    "    # add a column of 1s in index 0\n",
    "    feature_data = np.hstack((np.matrix(np.ones(feature_data.shape[0])).T, feature_data)) \n",
    "    y = copy.deepcopy(data.iloc[train])\n",
    "    y = y.iloc[:,target_class]\n",
    "    y = y.values\n",
    "    # initial beta values \n",
    "    betas = np.matrix(np.zeros(feature_data.shape[1]))\n",
    "    # use gradient descent to get final beta values\n",
    "    betas = gradient_descent(feature_data, y, betas) \n",
    "    # get the normalized featureset for the test data\n",
    "    test_data = copy.deepcopy(data)\n",
    "    test_data = test_data.iloc[test]\n",
    "    test_data = test_data.iloc[:, features]\n",
    "    test_data = test_data.values\n",
    "    test_data = normalize(test_data)\n",
    "    # add a column of 1s in index 0\n",
    "    test_data = np.hstack((np.matrix(np.ones(test_data.shape[0])).T, test_data)) \n",
    "    # calculate the target values for our test data\n",
    "    feature_probabilities = 1.0/(1 + np.exp(-np.dot(test_data, betas.T))) \n",
    "    sum_probabilities = feature_probabilities.sum(axis=1)\n",
    "    target_classes = np.where(sum_probabilities >= .5, 1, 0) \n",
    "    target_classes = np.squeeze(target_classes) \n",
    "    \n",
    "    return target_classes, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to normalize features in the data\n",
    "# used for continous features\n",
    "# feature_data is the data frame with just the features needing normalized\n",
    "# returns normed_features which is the data frame with normalized features\n",
    "def normalize(feature_data): \n",
    "    feature_mins = np.min(feature_data, axis = 0) \n",
    "    feature_maxes = np.max(feature_data, axis = 0) \n",
    "    feature_range = feature_maxes - feature_mins \n",
    "    normed_features = 1 - ((feature_maxes - feature_data)/feature_range) \n",
    "    return normed_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the cost of a particular beta set\n",
    "# betas is the betas being calculated for\n",
    "# feature_data is the data frame with just the features\n",
    "# y is the listing of target classes for the records from feature_data\n",
    "def cost(betas, feature_data, y): \n",
    "    sigmoid_result = 1.0/(1 + np.exp(-np.dot(feature_data, betas.T)))  \n",
    "    y = np.squeeze(y) \n",
    "    # uses log likelihood\n",
    "    first_step = y * np.log(sigmoid_result) \n",
    "    second_step = (1 - y) * np.log(1 - sigmoid_result) \n",
    "    final_cost =  -first_step - second_step \n",
    "    return np.mean(final_cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to use gradient descent to train the betas\n",
    "# feature_data is the data frame with just the features\n",
    "# y is the listing of target classes for the records from feature_data\n",
    "# betas is the list of initial betas passed\n",
    "# returns new betas learned by method\n",
    "def gradient_descent(feature_data, y, betas): \n",
    "    converge_delta = 0.001\n",
    "    learning_rate = 0.01\n",
    "    new_cost = cost(betas, feature_data, y) \n",
    "    change_cost = 1\n",
    "    counter = 1\n",
    "      \n",
    "    while(change_cost > converge_delta): \n",
    "        old_cost = new_cost \n",
    "        betas = betas - (learning_rate * log_gradient(betas, feature_data, y)) \n",
    "        new_cost = cost(betas, feature_data, y) \n",
    "        change_cost = old_cost - new_cost \n",
    "        counter += 1\n",
    "      \n",
    "    return betas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the log gradient\n",
    "# betas is the list of betas being calcualted for\n",
    "# feature_data is the data frame with just the features\n",
    "# y is the listing of target classes for the records from feature_data\n",
    "def log_gradient(betas, feature_data, y): \n",
    "    initial_calculation = (1.0/(1 + np.exp(-np.dot(feature_data, betas.T)))) - y.reshape(feature_data.shape[0], -1)\n",
    "    final_calculation = np.dot(initial_calculation.T, feature_data) \n",
    "    return final_calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Naive_Bayes\"></a>\n",
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "# data is the dataset to be used for the algorithm\n",
    "# test is a list of indices from data to be used for testing\n",
    "# train is a list of indices from data to be used for training\n",
    "# target_class is the index of the target class feature\n",
    "# attributes is a list of the indicies for the features to be used \n",
    "# returns target_classes, which is a list that contains the target class calculated for each test index\n",
    "# returns new_model, which is a list of the probabilities trained for the naive bayes model\n",
    "def naive_bayes_train(data, test, train, target_class, attributes):\n",
    "    num_records = len(train)\n",
    "    num_attributes = len(attributes)\n",
    "    model_part = [0] * (num_attributes+1)\n",
    "    new_model = [copy.deepcopy(model_part),copy.deepcopy(model_part),copy.deepcopy(model_part),copy.deepcopy(model_part)]\n",
    "    count_1s = 1\n",
    "    count_0s = 1\n",
    "    \n",
    "    #Model of form\n",
    "    # P(0) / P(xi = 0)\n",
    "    # ---- / P(xi = 1)\n",
    "    # P(1) / P(xi = 0)\n",
    "    # ---- / P(xi = 1)\n",
    "    for index_test in range(num_records): #Loop through all records in data\n",
    "        #add 1 to model for every datapoint \n",
    "        curr_record = data.iloc[[train[index_test]]] #pull current record data\n",
    "        #first add for when target_class = 1\n",
    "        if(curr_record.iloc[0,target_class]==1):\n",
    "            for index in range(num_attributes): #Loop through each attribute\n",
    "                if(curr_record.iloc[0,attributes[index]]==1):\n",
    "                    new_model[3][index+1] = new_model[3][index+1] +1\n",
    "                else:\n",
    "                    new_model[2][index+1] = new_model[2][index+1] +1\n",
    "            new_model[2][0] = new_model[2][0] +1\n",
    "            count_1s = count_1s +1\n",
    "                \n",
    "        #then add for when target_class = 0\n",
    "        else:\n",
    "            for index in range(num_attributes): #Loop through each attribute \n",
    "                if(curr_record.iloc[0,attributes[index]]==0):\n",
    "                    new_model[0][index+1] = new_model[0][index+1] +1\n",
    "                else:\n",
    "                    new_model[1][index+1]= new_model[1][index+1] +1\n",
    "            new_model[0][0] = new_model[0][0] +1\n",
    "            count_0s = count_0s+1\n",
    "\n",
    "    \n",
    "    new_model[0][0] = new_model[0][0] / num_records\n",
    "    new_model[2][0] = new_model[2][0] / num_records\n",
    "    for index in range(num_attributes):\n",
    "        new_model[0][index+1] = (new_model[0][index+1] + 0.01) / count_0s\n",
    "        new_model[1][index+1] = (new_model[1][index+1] + 0.01) / count_0s\n",
    "        new_model[2][index+1] = (new_model[2][index+1] + 0.01) / count_1s\n",
    "        new_model[3][index+1] = (new_model[3][index+1] + 0.01) / count_1s\n",
    "   \n",
    "    test_classes = naive_bayes_test(data, test, target_class, attributes, new_model)\n",
    "    return test_classes, new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to predict target class based off naive bayes model\n",
    "# data is the dataset to be used for the algorithm\n",
    "# test is a list of indices from data to be used for testing\n",
    "# target_class is the index of the target class feature\n",
    "# attributes is a list of the indicies for the features to be used \n",
    "# model is the naive bayes model\n",
    "# returns test_classes, which is a list that contains the target class calculated for each test index\n",
    "def naive_bayes_test(data, test, target_class, attributes, model):\n",
    "    num_records = len(test)\n",
    "    num_attributes = len(attributes)\n",
    "    test_classes = []\n",
    "\n",
    "    #Model of form\n",
    "    # P(0) / P(xi = 0)\n",
    "    # ---- / P(xi = 1)\n",
    "    # P(1) / P(xi = 0)\n",
    "    # ---- / P(xi = 1)\n",
    "    for index_test in range(num_records): #Loop through all records in data\n",
    "        curr_record = data.iloc[[test[index_test]]] #pull current record data\n",
    "        p_0 = model[0][0]\n",
    "        p_1 = model[2][0]\n",
    "        for index in range(num_attributes): #Loop through each attribute\n",
    "            if(curr_record.iloc[0,attributes[index]]==1):\n",
    "                p_0 = p_0 * model[2][index+1]\n",
    "                p_1 = p_1 * model[3][index+1]\n",
    "            else:\n",
    "                p_0 = p_0 * model[0][index+1]\n",
    "                p_1 = p_1 * model[1][index+1]\n",
    "                \n",
    "        #determine class and add to count_correct if correct\n",
    "        if(p_0>p_1):\n",
    "            test_classes.append(0)\n",
    "        else:\n",
    "            test_classes.append(1)\n",
    "    \n",
    "    return test_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Processing\"></a>\n",
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_metrics, breast_cancer_model, bc_nb_output = five_fold_cross_val(breast_cancer_binomial, breast_cancer_target_class, breast_cancer_features, naive_bayes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(bc_nb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.014705882352941176, 0.051470588235294115, 0.007352941176470588, 0.03676470588235294, 0.03676470588235294]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3656307129798903, 0.1294029850746269, 0.15427860696517415, 0.13935323383084577, 0.21398009950248756, 0.2587562189054726, 0.16422885572139304, 0.1891044776119403, 0.17915422885572138, 0.5423383084577115], [0, 0.8657213930348259, 0.8408457711442786, 0.8557711442786069, 0.7811442786069651, 0.73636815920398, 0.8308955223880596, 0.8060199004975124, 0.8159701492537313, 0.45278606965174134], [0.6343692870201096, 0.7356609195402298, 0.9712931034482758, 0.9483045977011494, 0.8937068965517241, 0.9569252873563218, 0.9483045977011494, 0.9483045977011494, 0.939683908045977, 0.9770402298850575], [0, 0.2615229885057471, 0.02589080459770115, 0.04887931034482759, 0.10347701149425287, 0.040258620689655175, 0.04887931034482759, 0.04887931034482759, 0.0575, 0.02014367816091954]]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_metrics, iris_model, iris_nb_output = five_fold_cross_val(iris_binomial, 5, iris_features, naive_bayes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(iris_nb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.06666666666666667, 0.03333333333333333, 0.06666666666666667, 0.03333333333333333]\n"
     ]
    }
   ],
   "source": [
    "print(iris_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.675, 0.3171951219512195, 0.7196341463414634, 0.08548780487804877, 0.12207317073170731], [0, 0.6708536585365853, 0.2684146341463415, 0.9025609756097561, 0.8659756097560977], [0.325, 0.97525, 0.17525, 0.97525, 0.97525], [0, 0.00025, 0.8002499999999999, 0.00025, 0.00025]]\n"
     ]
    }
   ],
   "source": [
    "print(iris_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_metrics, glass_model, glass_nb_output = five_fold_cross_val(glass_binomial, 15, glass_features, naive_bayes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(glass_nb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.023809523809523808, 0.047619047619047616, 0.0, 0.023809523809523808, 0.023809523809523808]\n"
     ]
    }
   ],
   "source": [
    "print(glass_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9476744186046512, 0.6525000000000001, 0.5671341463414634, 0.2622560975609756, 0.5854268292682927, 0.39640243902439026, 0.36591463414634146, 0.7256707317073171, 0.8415243902439024, 0.6890853658536585], [0, 0.3415243902439024, 0.42689024390243907, 0.7317682926829269, 0.4085975609756098, 0.5976219512195122, 0.628109756097561, 0.2683536585365853, 0.1525, 0.3049390243902439], [0.05232558139534884, 0.301, 0.001, 0.901, 0.301, 0.301, 0.901, 0.20099999999999998, 0.901, 0.901], [0, 0.601, 0.901, 0.001, 0.601, 0.601, 0.001, 0.701, 0.001, 0.001]]\n"
     ]
    }
   ],
   "source": [
    "print(glass_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_metrics, votes_model, votes_nb_output = five_fold_cross_val(votes, 0, votes_features, naive_bayes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(votes_nb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13793103448275862, 0.06896551724137931, 0.06896551724137931, 0.06896551724137931, 0.13793103448275862]\n"
     ]
    }
   ],
   "source": [
    "print(votes_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3879310344827586, 0.81625, 0.56625, 0.8603676470588235, 0.029485294117647057, 0.06625, 0.11036764705882353, 0.7647794117647059, 0.8456617647058824, 0.8750735294117648, 0.4265441176470588, 0.8603676470588235, 0.17654411764705882, 0.1986029411764706, 0.051544117647058824, 0.9044852941176471, 0.41919117647058823], [0, 0.17654411764705882, 0.4265441176470588, 0.1324264705882353, 0.9633088235294117, 0.9265441176470589, 0.8824264705882353, 0.22801470588235295, 0.1471323529411765, 0.11772058823529413, 0.56625, 0.1324264705882353, 0.81625, 0.7941911764705882, 0.9412499999999999, 0.08830882352941176, 0.5736029411764706], [0.6120689655172413, 0.3878971962616823, 0.5514485981308411, 0.11686915887850469, 0.9533177570093457, 0.817803738317757, 0.5748130841121496, 0.215, 0.14957943925233644, 0.2850934579439252, 0.5187383177570094, 0.5327570093457944, 0.8598598130841121, 0.7243457943925233, 0.6869626168224299, 0.3832242990654206, 0.3645327102803739], [0, 0.6075233644859813, 0.4439719626168225, 0.8785514018691588, 0.042102803738317755, 0.17761682242990653, 0.42060747663551407, 0.7804205607476635, 0.8458411214953271, 0.7103271028037382, 0.4766822429906542, 0.46266355140186916, 0.13556074766355142, 0.27107476635514016, 0.3084579439252337, 0.612196261682243, 0.6308878504672897]]\n"
     ]
    }
   ],
   "source": [
    "print(votes_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean_metrics, soybean_model, soybean_nb_output = five_fold_cross_val(soybean_binomial, 36, soybean_features, naive_bayes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(soybean_nb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(soybean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7631578947368421, 0.06699999999999999, 0.46699999999999997, 0.7336666666666667, 0.4003333333333333, 0.9670000000000001, 0.46699999999999997, 0.5670000000000001, 0.5003333333333333, 0.267, 0.8003333333333333, 0.5336666666666667, 0.5670000000000001, 0.7003333333333334, 0.267, 0.8003333333333333, 0.9670000000000001, 0.9670000000000001, 0.267, 0.5336666666666667], [0, 0.9003333333333334, 0.5003333333333333, 0.23366666666666666, 0.5670000000000001, 0.0003333333333333333, 0.5003333333333333, 0.4003333333333333, 0.46699999999999997, 0.7003333333333334, 0.16699999999999998, 0.43366666666666664, 0.4003333333333333, 0.267, 0.7003333333333334, 0.16699999999999998, 0.0003333333333333333, 0.0003333333333333333, 0.7003333333333334, 0.43366666666666664], [0.23684210526315788, 0.901, 0.001, 0.40099999999999997, 0.40099999999999997, 0.001, 0.901, 0.40099999999999997, 0.601, 0.001, 0.701, 0.901, 0.001, 0.901, 0.901, 0.901, 0.001, 0.001, 0.901, 0.901], [0, 0.001, 0.901, 0.501, 0.501, 0.901, 0.001, 0.501, 0.301, 0.901, 0.20099999999999998, 0.001, 0.901, 0.001, 0.001, 0.001, 0.901, 0.901, 0.001, 0.001]]\n"
     ]
    }
   ],
   "source": [
    "print(soybean_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_log_metrics, breast_cancer_log_model, breast_cancer_output = five_fold_cross_val(breast_cancer_log, breast_cancer_target_class, breast_cancer_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
      " 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0\n",
      " 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
      " 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03676470588235294, 0.03676470588235294, 0.04411764705882353, 0.022058823529411766, 0.022058823529411766]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.14383053 -5.56185825 -1.29464269 -1.92306616 -2.41674833 -0.5385677\n",
      "  -3.41618193 -3.14474547 -1.73555005 -2.42626869]]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_log_metrics, iris_log_model, iris_output = five_fold_cross_val(iris_log, 5, iris_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(iris_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(iris_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.88054314 -3.24987857  5.79453883 -7.87053119 -7.57496416]]\n"
     ]
    }
   ],
   "source": [
    "print(iris_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_log_metrics, glass_log_model, glass_output = five_fold_cross_val(glass_log, 15, glass_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(glass_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047619047619047616, 0.07142857142857142, 0.07142857142857142, 0.16666666666666666, 0.023809523809523808]\n"
     ]
    }
   ],
   "source": [
    "print(glass_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.56568035  -3.59112973   9.92041539  -2.69954827  -1.84912899\n",
      "    0.11651702  -7.26233494  -0.41371806 -11.77698603  -5.80958883]]\n"
     ]
    }
   ],
   "source": [
    "print(glass_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_log_metrics, votes_log_model, votes_output = five_fold_cross_val(votes, 0, votes_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(votes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011494252873563218, 0.034482758620689655, 0.06896551724137931, 0.034482758620689655, 0.034482758620689655]\n"
     ]
    }
   ],
   "source": [
    "print(votes_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21249096  0.37492762  1.63462776  3.61962031 -7.32366436 -0.3287923\n",
      "   1.1798216  -0.71941434 -0.53903372  2.21084609 -1.75585308  3.35425818\n",
      "  -0.99305289  0.4689131   0.88643717  1.95781593 -0.56544845]]\n"
     ]
    }
   ],
   "source": [
    "print(votes_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunsyray/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n",
      "/home/sunsyray/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    }
   ],
   "source": [
    "soybean_log_metrics, soybean_log_model, soybean_output = five_fold_cross_val(soybean_log, 36, soybean_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(soybean_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.2222222222222222]\n"
     ]
    }
   ],
   "source": [
    "print(soybean_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48061779 -2.02620935  0.65700547  0.43620733 -0.2719909   0.78127257\n",
      "  -0.93130679 -0.13679937 -0.42946579 -0.0579817   0.05504234 -1.32773283\n",
      "   0.71477194 -0.6399324  -1.42650624 -0.23620877  1.786354    1.786354\n",
      "  -1.62703939 -1.17962496]]\n"
     ]
    }
   ],
   "source": [
    "print(soybean_log_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
