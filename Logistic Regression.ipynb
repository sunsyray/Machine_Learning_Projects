{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to convert features to binomial\n",
    "# data is the dataset to be used for the algorithm\n",
    "# features is a list of the indicies for the features\n",
    "# return data_binomial, which is the dataset with the features converted to binomial\n",
    "def binomial_features(data, features):\n",
    "    data_binomial = copy.deepcopy(data)\n",
    "    for index in features:\n",
    "        column_name = data_binomial.columns[index]\n",
    "        mean = data_binomial[column_name].mean()\n",
    "        data_ltm = data_binomial[column_name] < mean\n",
    "        data_binomial.loc[data_ltm, column_name] = 0\n",
    "        data_gtm = data_binomial[column_name] >= mean\n",
    "        data_binomial.loc[data_gtm, column_name] = 1\n",
    "    return data_binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code performs Logistic Regression on datasets from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/index.php\n",
    "\n",
    "\n",
    "[Datasets](#Datasets):\n",
    "\n",
    "[Iris](#Iris)- 3 classes    \n",
    "[Glass](#Glass)- 7 classes  \n",
    "[Breast Cancer](#Breast_Cancer)- 2 classes  \n",
    "[Congressional Voting Records](#Votes)- 2 classes  \n",
    "\n",
    "[Cross Validation](#Cross_Val)  \n",
    "[Logistic Regression](#Log_Regress)  \n",
    "[Models](#Models)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Datasets\"></a>\n",
    "# Datasets\n",
    "\n",
    "<a id=\"Iris\"></a>\n",
    "## Iris\n",
    "\n",
    "Dummies for different Iris Classes:  \n",
    "iris_class_Iris-setosa       \n",
    "iris_class_Iris-versicolor    \n",
    "iris_class_Iris-virginica  \n",
    "Target Class Indices: [5,6,7]  \n",
    "Feature Indices: [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv( \"iris.data\", header=None, names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width','iris_class'])\n",
    "iris_log = pd.read_csv( \"iris.data\", header=None, names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width','iris_class'])\n",
    "iris = pd.concat([iris, pd.get_dummies(iris[\"iris_class\"],prefix = 'iris_class')], axis=1)\n",
    "iris_log = pd.concat([iris_log, pd.get_dummies(iris[\"iris_class\"],prefix = 'iris_class')], axis=1)\n",
    "iris_features = [0,1,2,3]\n",
    "iris_binomial = binomial_features(iris, iris_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Glass\"></a>\n",
    "## Glass\n",
    "\n",
    "Class Indices: [11,12,13,14,15,16]  \n",
    "Feature Indices: [1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = pd.read_csv( \"glass.data\", header=None, names = ['ID','RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','glass_class'])\n",
    "glass_log = pd.read_csv( \"glass.data\", header=None, names = ['ID','RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','glass_class'])\n",
    "glass = pd.concat([glass, pd.get_dummies(glass[\"glass_class\"],prefix = 'glass_class')], axis=1)\n",
    "glass_log = pd.concat([glass_log, pd.get_dummies(glass[\"glass_class\"],prefix = 'glass_class')], axis=1)\n",
    "glass_features = [1,2,3,4,5,6,7,8,9]\n",
    "glass_binomial = binomial_features(glass, glass_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Breast_Cancer\"></a>\n",
    "## Breast Cancer\n",
    "\n",
    "Remove 16 records with missing data.  \n",
    "Class 2 changed to 1  \n",
    "Class 4 changed to 0  \n",
    "Class Index: 10  \n",
    "Features: [1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = pd.read_csv( \"breast-cancer-wisconsin.data\", header=None, names = ['ID','clump_thick','unif_cell_size','unif_cell_shape','marg_adh','single_epit_cell_size','bare_nuclei','bland_chromatin','normal_nucleoli','mitosis','cancer_class'])\n",
    "breast_cancer_log = pd.read_csv( \"breast-cancer-wisconsin.data\", header=None, names = ['ID','clump_thick','unif_cell_size','unif_cell_shape','marg_adh','single_epit_cell_size','bare_nuclei','bland_chromatin','normal_nucleoli','mitosis','cancer_class'])\n",
    "breast_cancer['cancer_class'] = breast_cancer['cancer_class'].map({2: 1, 4: 0})\n",
    "breast_cancer_log['cancer_class'] = breast_cancer_log['cancer_class'].map({2: 1, 4: 0})\n",
    "breast_cancer=breast_cancer[breast_cancer.bare_nuclei != '?']\n",
    "breast_cancer.bare_nuclei = breast_cancer.bare_nuclei.astype(int)\n",
    "breast_cancer_log=breast_cancer_log[breast_cancer_log.bare_nuclei != '?']\n",
    "breast_cancer_log.bare_nuclei = breast_cancer_log.bare_nuclei.astype(int)\n",
    "breast_cancer_features = [1,2,3,4,5,6,7,8,9]\n",
    "breast_cancer_target_class = 10\n",
    "breast_cancer_binomial = binomial_features(breast_cancer, breast_cancer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Votes\"></a>\n",
    "## Votes\n",
    "\n",
    "Changed all ? to 'n', 'n' to 0, 'y' to 1.  \n",
    "Democrats = 1  \n",
    "Republicans = 0  \n",
    "Class Index: 0  \n",
    "Feature Indices: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = pd.read_csv( \"house-votes-84.data\", header=None, names = ['votes_class','handicapped_infants','water_project','adoption_budget_resol','physician_fee_freeze','el_salv_aid','rel_groups_in_schools','anti_satellite_test_ban','aid_to_nica_contras','mx_missile','immigration','synfuels','education_spending','superfund_right_to_sue','crime','duty_free_exports','export_admin_act_south_africa'])\n",
    "votes = votes.replace('y', 1)\n",
    "votes = votes.replace('n', 0)\n",
    "votes = votes.replace('?', 0)\n",
    "votes = votes.replace('democrat', 1)\n",
    "votes = votes.replace('republican', 0)\n",
    "votes_features = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Cross_Val\"></a>\n",
    "## 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross Validation\n",
    "# data is the dataset to be used for the algorithm\n",
    "# target_class is the index of the target class feature\n",
    "# features is a list of the indicies for the features to be used to measure distance\n",
    "# Algorithm is which algorithm is chosen to be used\n",
    "def five_fold_cross_val(data, target_class, features, Algorithm):\n",
    "    metrics= []\n",
    "    list_of_indices = list(range(len(data)))\n",
    "    n_data = len(data) #this is the number of records in the data\n",
    "    test_size = int(n_data/5) #this is the number of records in each test set\n",
    "    df = copy.deepcopy(data)\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # a dataframe with shuffled records\n",
    "    df.sort_values(by= df.columns[target_class]) # sorts the data by the target class\n",
    "    \n",
    "    split1 = []\n",
    "    split2 = []\n",
    "    split3 = []\n",
    "    split4 = []\n",
    "    split5 = []\n",
    "    for count in range(test_size):\n",
    "        split1.append(5*count)\n",
    "        split2.append(5*count +1)\n",
    "        split3.append(5*count +2)\n",
    "        split4.append(5*count +3)\n",
    "        split5.append(5*count +4)\n",
    "    \n",
    "    # test split1\n",
    "    split1_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split1):\n",
    "            split1_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes1, model1 = Algorithm(df, split1, split1_train, target_class, features)\n",
    "    output1 = performance(df, split1, test_classes1, target_class) \n",
    "    metrics.append(output1)\n",
    "    \n",
    "    # test split2\n",
    "    split2_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split2):\n",
    "            split2_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes2, model2 = Algorithm(df, split2, split2_train, target_class, features)\n",
    "    output2 = performance(df, split2, test_classes2, target_class)     \n",
    "    metrics.append(output2)\n",
    "    \n",
    "    # test split3\n",
    "    split3_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split3):\n",
    "            split3_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes3, model3 = Algorithm(df, split3, split3_train, target_class, features)\n",
    "    output3 = performance(df, split3, test_classes3, target_class)    \n",
    "    metrics.append(output3)\n",
    "    \n",
    "    # test split4\n",
    "    split4_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split4):\n",
    "            split4_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes4, model4 = Algorithm(df, split4, split4_train, target_class, features)\n",
    "    output4 = performance(df, split4, test_classes4, target_class)    \n",
    "    metrics.append(output4)\n",
    "    \n",
    "    # test split5\n",
    "    split5_train = []\n",
    "    for index in range(n_data):\n",
    "        if(index not in split5):\n",
    "            split5_train.append(index)\n",
    "        # end if\n",
    "    # end for\n",
    "    # run algorithm\n",
    "    test_classes5, model5 = Algorithm(df, split5, split5_train, target_class, features)\n",
    "    output5 = performance(df, split5, test_classes5, target_class)  \n",
    "    metrics.append(output5)  \n",
    "        \n",
    "    return metrics, model1, test_classes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "# data is the dataset to be used for the algorithm\n",
    "# test is a list of indices from data to be used for testing\n",
    "# test_classes is a list with the class for the test dataset that includes the estimate for the target_class\n",
    "# features is a list of the indicies for the features to be used to measure distance\n",
    "# target_class is the index of the target class feature\n",
    "def performance(data, test, test_classes, target_class):\n",
    "    n_data = len(test) # this is the number of records to be tested\n",
    "    curr_perf = 0\n",
    "\n",
    "    # classification error\n",
    "    for record in range(n_data): # record is index in test \n",
    "        test_record = data.iloc[[test[record]]] # pulls record to test\n",
    "        if(test_record.iloc[0,target_class] != test_classes[record]):\n",
    "            curr_perf = curr_perf + 1\n",
    "        # end if\n",
    "    # end for\n",
    "    curr_perf = curr_perf / n_data\n",
    "\n",
    "    return curr_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Log_Regress\"></a>\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# data is the dataset to be used for the algorithm\n",
    "# test is a list of indices from data to be used for testing\n",
    "# train is a list of indices from data to be used for training\n",
    "# target_class is the index of the target class feature\n",
    "# features is a list of the indicies for the features to be used to measure distance\n",
    "# returns target_classes, which is a list that contains the target class calculated for each test index\n",
    "# returns betas, which is a list of the betas trained for the logistic regression model\n",
    "def logistic_regression_train(data, test, train, target_class, features):\n",
    "    # get the normalized featureset for the train data\n",
    "    train_data = copy.deepcopy(data)\n",
    "    train_data = train_data.iloc[train]\n",
    "    train_data = train_data.iloc[:,features]\n",
    "    train_data = train_data.values\n",
    "    feature_data = normalize(train_data)\n",
    "    # add a column of 1s in index 0\n",
    "    feature_data = np.hstack((np.matrix(np.ones(feature_data.shape[0])).T, feature_data)) \n",
    "    y = copy.deepcopy(data.iloc[train])\n",
    "    y = y.iloc[:,target_class]\n",
    "    y = y.values\n",
    "    # initial beta values \n",
    "    betas = np.matrix(np.zeros(feature_data.shape[1]))\n",
    "    # use gradient descent to get final beta values\n",
    "    betas = gradient_descent(feature_data, y, betas) \n",
    "    # get the normalized featureset for the test data\n",
    "    test_data = copy.deepcopy(data)\n",
    "    test_data = test_data.iloc[test]\n",
    "    test_data = test_data.iloc[:, features]\n",
    "    test_data = test_data.values\n",
    "    test_data = normalize(test_data)\n",
    "    # add a column of 1s in index 0\n",
    "    test_data = np.hstack((np.matrix(np.ones(test_data.shape[0])).T, test_data)) \n",
    "    # calculate the target values for our test data\n",
    "    feature_probabilities = 1.0/(1 + np.exp(-np.dot(test_data, betas.T))) \n",
    "    sum_probabilities = feature_probabilities.sum(axis=1)\n",
    "    target_classes = np.where(sum_probabilities >= .5, 1, 0) \n",
    "    target_classes = np.squeeze(target_classes) \n",
    "    \n",
    "    return target_classes, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to normalize features in the data\n",
    "# used for continous features\n",
    "# feature_data is the data frame with just the features needing normalized\n",
    "# returns normed_features which is the data frame with normalized features\n",
    "def normalize(feature_data): \n",
    "    feature_mins = np.min(feature_data, axis = 0) \n",
    "    feature_maxes = np.max(feature_data, axis = 0) \n",
    "    feature_range = feature_maxes - feature_mins \n",
    "    normed_features = 1 - ((feature_maxes - feature_data)/feature_range) \n",
    "    return normed_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the cost of a particular beta set\n",
    "# betas is the betas being calculated for\n",
    "# feature_data is the data frame with just the features\n",
    "# y is the listing of target classes for the records from feature_data\n",
    "def cost(betas, feature_data, y): \n",
    "    sigmoid_result = 1.0/(1 + np.exp(-np.dot(feature_data, betas.T)))  \n",
    "    y = np.squeeze(y) \n",
    "    # uses log likelihood\n",
    "    first_step = y * np.log(sigmoid_result) \n",
    "    second_step = (1 - y) * np.log(1 - sigmoid_result) \n",
    "    final_cost =  -first_step - second_step \n",
    "    return np.mean(final_cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to use gradient descent to train the betas\n",
    "# feature_data is the data frame with just the features\n",
    "# y is the listing of target classes for the records from feature_data\n",
    "# betas is the list of initial betas passed\n",
    "# returns new betas learned by method\n",
    "def gradient_descent(feature_data, y, betas): \n",
    "    converge_delta = 0.001\n",
    "    learning_rate = 0.01\n",
    "    new_cost = cost(betas, feature_data, y) \n",
    "    change_cost = 1\n",
    "    counter = 1\n",
    "      \n",
    "    while(change_cost > converge_delta): \n",
    "        old_cost = new_cost \n",
    "        betas = betas - (learning_rate * log_gradient(betas, feature_data, y)) \n",
    "        new_cost = cost(betas, feature_data, y) \n",
    "        change_cost = old_cost - new_cost \n",
    "        counter += 1\n",
    "      \n",
    "    return betas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the log gradient\n",
    "# betas is the list of betas being calcualted for\n",
    "# feature_data is the data frame with just the features\n",
    "# y is the listing of target classes for the records from feature_data\n",
    "def log_gradient(betas, feature_data, y): \n",
    "    initial_calculation = (1.0/(1 + np.exp(-np.dot(feature_data, betas.T)))) - y.reshape(feature_data.shape[0], -1)\n",
    "    final_calculation = np.dot(initial_calculation.T, feature_data) \n",
    "    return final_calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Processing\"></a>\n",
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "\n",
    "Performance is based on Classification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_log_metrics, breast_cancer_log_model, breast_cancer_output = five_fold_cross_val(breast_cancer_log, breast_cancer_target_class, breast_cancer_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1\n",
      " 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
      " 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.029411764705882353, 0.04411764705882353, 0.058823529411764705, 0.014705882352941176, 0.022058823529411766]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.04270679 -4.87595501 -0.1080613  -2.26185057 -4.02614809 -1.41508764\n",
      "  -3.59446137 -3.24903743 -1.45842438 -3.40202366]]\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_log_metrics, iris_log_model, iris_output = five_fold_cross_val(iris_log, 5, iris_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(iris_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(iris_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.54770755 -3.30689559  5.83134949 -7.95385444 -7.56531688]]\n"
     ]
    }
   ],
   "source": [
    "print(iris_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_log_metrics, glass_log_model, glass_output = five_fold_cross_val(glass_log, 15, glass_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(glass_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.047619047619047616, 0.11904761904761904, 0.07142857142857142, 0.023809523809523808]\n"
     ]
    }
   ],
   "source": [
    "print(glass_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.96340581  -3.92663025   9.79763478  -2.37918345  -0.46490111\n",
      "    0.14772643  -9.7627006    0.68073363 -13.84404376  -6.96701087]]\n"
     ]
    }
   ],
   "source": [
    "print(glass_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_log_metrics, votes_log_model, votes_output = five_fold_cross_val(votes, 0, votes_features, logistic_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
      " 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1\n",
      " 0 0 0 1 0 0 0 1 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(votes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04597701149425287, 0.04597701149425287, 0.034482758620689655, 0.034482758620689655, 0.05747126436781609]\n"
     ]
    }
   ],
   "source": [
    "print(votes_log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22141687  0.10644634  1.10119753  3.22181472 -8.66302572  1.50500071\n",
      "   2.15606686 -0.19888698 -0.19466821  2.14107699 -1.64371001  4.36716674\n",
      "  -1.63245533 -1.18256009  1.4078778   2.16473481 -0.40465216]]\n"
     ]
    }
   ],
   "source": [
    "print(votes_log_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
